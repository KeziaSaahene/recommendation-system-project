{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGn6QeSmtrsatZvhLQu1XY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "438b5bd90a7e44399fa42dfe2e1f8f74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7e2957ed8df4eba818e2fa6c7bab0be",
              "IPY_MODEL_17317f047616477fab50f412b4200a51",
              "IPY_MODEL_f61e81ff500f4a64b4d3d01ec6e7201f"
            ],
            "layout": "IPY_MODEL_dc35f7b9920b4d73906fdc28ec0e0e86"
          }
        },
        "d7e2957ed8df4eba818e2fa6c7bab0be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca0c043c0abd4d95b4cee2bcdb4ebce8",
            "placeholder": "​",
            "style": "IPY_MODEL_225ccb3aa3e74b718b2105e46caa4950",
            "value": "100%"
          }
        },
        "17317f047616477fab50f412b4200a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d59a8abf4534b0ab9f3c2826dbdba6b",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dca0f71a433b473cbaa30554f16f682d",
            "value": 5
          }
        },
        "f61e81ff500f4a64b4d3d01ec6e7201f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1edd1f50ee684409a4e0344c118cb2b1",
            "placeholder": "​",
            "style": "IPY_MODEL_48acc8ab59814ad88608318ae40754cf",
            "value": " 5/5 [00:29&lt;00:00,  5.58s/it]"
          }
        },
        "dc35f7b9920b4d73906fdc28ec0e0e86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca0c043c0abd4d95b4cee2bcdb4ebce8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "225ccb3aa3e74b718b2105e46caa4950": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d59a8abf4534b0ab9f3c2826dbdba6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dca0f71a433b473cbaa30554f16f682d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1edd1f50ee684409a4e0344c118cb2b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48acc8ab59814ad88608318ae40754cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeziaSaahene/recommendation-system-project/blob/main/Recommendation_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install implicit lightgbm"
      ],
      "metadata": {
        "id": "vW2qUrvYJGfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X5QGTXJlJMbA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try importing implicit (ALS)\n",
        "try:\n",
        "    import implicit\n",
        "except ImportError:\n",
        "    implicit = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN7CeXrMKZAf",
        "outputId": "2f15c69a-d825-48ba-9265-33956efcacc2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/implicit/gpu/__init__.py:13: UserWarning: CUDA extension is built, but disabling GPU support because of 'Cuda Error: CUDA driver version is insufficient for CUDA runtime version (/tmp/pip-install-ldl5n_d8/implicit_ad68d5a7c50c471c92a4c5250807b5b3/./implicit/gpu/utils.h:71)'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubRFnbXGukRZ",
        "outputId": "18b9f19c-44fc-4733-98ce-220c8bbdff11"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/drive/MyDrive/Project 2/\""
      ],
      "metadata": {
        "id": "dDPqd8cA0tJC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading & Cleaning datasets"
      ],
      "metadata": {
        "id": "s_1Xt_m-1ICc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_value(val):\n",
        "    \"\"\"Convert 'n123' -> 123 (int), keep others unchanged.\"\"\"\n",
        "    if isinstance(val, str) and val.startswith(\"n\"):\n",
        "        try:\n",
        "            return int(val[1:])\n",
        "        except ValueError:\n",
        "            return val\n",
        "    return val\n",
        "\n",
        "MAX_EVENTS = None  # limit for demo speed"
      ],
      "metadata": {
        "id": "j0tlHlgcNN9s"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load events\n",
        "events = pd.read_csv(base_path + \"events.csv\", nrows=MAX_EVENTS)\n",
        "# Load and clean item_properties\n",
        "item_props = pd.read_csv(base_path + \"item_properties_combined.csv\",\n",
        "                         names=[\"timestamp\", \"itemid\", \"property\", \"value\"],\n",
        "                         skiprows=1)  # Skip the header row\n",
        "item_props[\"value\"] = item_props[\"value\"].apply(clean_value)\n",
        "\n",
        "# Load category_tree\n",
        "category_tree = pd.read_csv(base_path + \"category_tree.csv\",\n",
        "                            names=[\"child\", \"parent\"],\n",
        "                            skiprows=1)  # also skip header row if needed\n",
        "\n",
        "print(f\"Events shape: {events.shape}\")\n",
        "print(f\"Item properties shape: {item_props.shape}\")\n",
        "print(f\"Category tree shape: {category_tree.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAujmq0U0_sp",
        "outputId": "6ffe98ff-6216-4467-f748-5b17d3c4912d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Events shape: (2756101, 5)\n",
            "Item properties shape: (9623154, 4)\n",
            "Category tree shape: (1669, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting Events timestamp to datetime\n",
        "events['timestamp'] = pd.to_datetime(events['timestamp'], unit='ms')"
      ],
      "metadata": {
        "id": "XEX-X_5p1rbg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename \"property\" to \"categoryid\"\n",
        "item_props = item_props.rename(columns={\"property\": \"categoryid\"})"
      ],
      "metadata": {
        "id": "U3V693ps2bRl"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show column names\n",
        "print(\"Events columns:\", events.columns)\n",
        "print(\"Items columns:\", item_props.columns)\n",
        "print(\"Category Tree columns:\", category_tree.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fW7a0m52Lt2",
        "outputId": "82b3e672-49d7-40e3-ba71-f154694e4d13"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Events columns: Index(['timestamp', 'visitorid', 'event', 'itemid', 'transactionid', 'weight',\n",
            "       'user_idx', 'item_idx'],\n",
            "      dtype='object')\n",
            "Items columns: Index(['timestamp', 'itemid', 'categoryid', 'value'], dtype='object')\n",
            "Category Tree columns: Index(['child', 'parent'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick preview"
      ],
      "metadata": {
        "id": "yJAuKuTP21H_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Events sample:\\n\", events.head())\n",
        "print(events.dtypes)\n",
        "print(\"\\nItems sample:\\n\", item_props.head())\n",
        "print(item_props.dtypes)\n",
        "print(\"\\nCategory Tree sample:\\n\", category_tree.head())\n",
        "print(category_tree.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELw62FUV25RT",
        "outputId": "0b00ea9e-05f1-4f36-8f02-27925ae3dede"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Events sample:\n",
            "                 timestamp  visitorid event  itemid  transactionid  weight  \\\n",
            "0 2015-06-02 05:02:12.117     257597  view  355908            NaN     1.0   \n",
            "1 2015-06-02 05:50:14.164     992329  view  248676            NaN     1.0   \n",
            "2 2015-06-02 05:13:19.827     111016  view  318965            NaN     1.0   \n",
            "3 2015-06-02 05:12:35.914     483717  view  253185            NaN     1.0   \n",
            "4 2015-06-02 05:02:17.106     951259  view  367447            NaN     1.0   \n",
            "\n",
            "   user_idx  item_idx  \n",
            "0    257597    179333  \n",
            "1    992329    125263  \n",
            "2    111016    160653  \n",
            "3    483717    127563  \n",
            "4    951259    185159  \n",
            "timestamp        datetime64[ns]\n",
            "visitorid                 int64\n",
            "event                    object\n",
            "itemid                    int64\n",
            "transactionid           float64\n",
            "weight                  float64\n",
            "user_idx                  int64\n",
            "item_idx                  int64\n",
            "dtype: object\n",
            "\n",
            "Items sample:\n",
            "              timestamp  itemid  categoryid                            value\n",
            "0  2015-06-28 03:00:00  460429  categoryid                             1338\n",
            "1  2015-09-06 03:00:00  206783         888          1116713 960601 n277.200\n",
            "2  2015-08-09 03:00:00  395014         400  n552.000 639502 n720.000 424566\n",
            "3  2015-05-10 03:00:00   59481         790                       n15360.000\n",
            "4  2015-05-17 03:00:00  156781         917                           828513\n",
            "timestamp     object\n",
            "itemid         int64\n",
            "categoryid    object\n",
            "value         object\n",
            "dtype: object\n",
            "\n",
            "Category Tree sample:\n",
            "    child  parent\n",
            "0   1016   213.0\n",
            "1    809   169.0\n",
            "2    570     9.0\n",
            "3   1691   885.0\n",
            "4    536  1691.0\n",
            "child       int64\n",
            "parent    float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Candidate Generation\n",
        "# -------------------------------\n",
        "def build_user_item_matrix(events_df):\n",
        "    weight_map = {\"view\": 1.0, \"addtocart\": 3.0, \"transaction\": 5.0}\n",
        "    events_df[\"weight\"] = events_df[\"event\"].map(weight_map).fillna(1.0)\n",
        "    user_enc = LabelEncoder()\n",
        "    item_enc = LabelEncoder()\n",
        "    events_df[\"user_idx\"] = user_enc.fit_transform(events_df[\"visitorid\"])\n",
        "    events_df[\"item_idx\"] = item_enc.fit_transform(events_df[\"itemid\"])\n",
        "    rows, cols, vals = events_df[\"user_idx\"].values, events_df[\"item_idx\"].values, events_df[\"weight\"].values\n",
        "    ui_matrix = sparse.coo_matrix((vals, (rows, cols)),\n",
        "                                  shape=(len(user_enc.classes_), len(item_enc.classes_)))\n",
        "    return ui_matrix.tocsr(), user_enc, item_enc\n",
        "\n",
        "ui_matrix, user_enc, item_enc = build_user_item_matrix(events)"
      ],
      "metadata": {
        "id": "0C2mbB75QUvj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ALS recommendations (if implicit is installed)\n",
        "if implicit is not None:\n",
        "    als_model = implicit.als.AlternatingLeastSquares(factors=32, iterations=5)\n",
        "    als_model.fit(ui_matrix.T)\n",
        "    user0_recs = als_model.recommend(0, ui_matrix[0], N=5)\n",
        "    print(\"ALS recs for first user:\", user0_recs)\n",
        "else:\n",
        "    print(\"implicit not installed, skipping ALS.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "438b5bd90a7e44399fa42dfe2e1f8f74",
            "d7e2957ed8df4eba818e2fa6c7bab0be",
            "17317f047616477fab50f412b4200a51",
            "f61e81ff500f4a64b4d3d01ec6e7201f",
            "dc35f7b9920b4d73906fdc28ec0e0e86",
            "ca0c043c0abd4d95b4cee2bcdb4ebce8",
            "225ccb3aa3e74b718b2105e46caa4950",
            "2d59a8abf4534b0ab9f3c2826dbdba6b",
            "dca0f71a433b473cbaa30554f16f682d",
            "1edd1f50ee684409a4e0344c118cb2b1",
            "48acc8ab59814ad88608318ae40754cf"
          ]
        },
        "id": "G3H_kwnIQrAf",
        "outputId": "4b46540e-4b06-4554-fa67-d657923f314d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/implicit/cpu/als.py:95: RuntimeWarning: OpenBLAS is configured to use 2 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
            "  check_blas_config()\n",
            "/usr/local/lib/python3.12/dist-packages/implicit/utils.py:164: ParameterWarning: Method expects CSR input, and was passed csc_matrix instead. Converting to CSR took 0.15790557861328125 seconds\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "438b5bd90a7e44399fa42dfe2e1f8f74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALS recs for first user: (array([684514, 163561, 518659, 388556, 890980], dtype=int32), array([2.1900421e-06, 2.0385316e-06, 1.7352656e-06, 1.7204412e-06,\n",
            "       1.7118198e-06], dtype=float32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics\n",
        "# -------------------------------\n",
        "def recall_at_k(true_items, pred_items, k):\n",
        "    return len(set(pred_items[:k]) & set(true_items)) / max(1, len(set(true_items)))\n",
        "\n",
        "def hit_rate_at_k(true_items, pred_items, k):\n",
        "    return int(len(set(pred_items[:k]) & set(true_items)) > 0)\n",
        "\n",
        "def ndcg_at_k(true_items, pred_items, k):\n",
        "    def dcg(scores):\n",
        "        return sum((2**s - 1) / np.log2(i+2) for i, s in enumerate(scores))\n",
        "    rels = [1 if p in true_items else 0 for p in pred_items[:k]]\n",
        "    return dcg(rels) / max(dcg(sorted(rels, reverse=True)), 1e-9)"
      ],
      "metadata": {
        "id": "I-FmLHM4Q6d9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 1 - Deep Learning Model"
      ],
      "metadata": {
        "id": "PYWZRMS4Raaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep Learning Model (Tuned CNN Only)\n",
        "# -------------------------------\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# --- Step 1: Prepare data ---\n",
        "target_prop = item_props[\"categoryid\"].value_counts().index[0]\n",
        "print(\"Target property for prediction:\", target_prop)\n",
        "\n",
        "# Build item->latest value for target_prop\n",
        "prop_df = item_props[item_props[\"categoryid\"] == target_prop].sort_values(\"timestamp\")\n",
        "item_latest_prop = prop_df.groupby(\"itemid\")[\"value\"].last().to_dict()\n",
        "\n",
        "X_texts, y_labels = [], []\n",
        "for _, row in events[events[\"event\"] == \"addtocart\"].iterrows():\n",
        "    user_hist = events[(events[\"visitorid\"] == row[\"visitorid\"]) &\n",
        "                       (events[\"timestamp\"] < row[\"timestamp\"]) &\n",
        "                       (events[\"event\"] == \"view\")]\n",
        "    tokens = [str(item_latest_prop.get(i, \"\")) for i in user_hist[\"itemid\"].values]\n",
        "    label = item_latest_prop.get(row[\"itemid\"])\n",
        "    if label and tokens:\n",
        "        X_texts.append(\" \".join(tokens))\n",
        "        y_labels.append(label)\n",
        "\n",
        "# Filter to frequent classes only (adaptive min frequency)\n",
        "counts = Counter(y_labels)\n",
        "min_freq = 10  # lower to keep more classes for small samples\n",
        "top_classes = {cls for cls, cnt in counts.items() if cnt >= min_freq}\n",
        "filtered = [(txt, lbl) for txt, lbl in zip(X_texts, y_labels) if lbl in top_classes]\n",
        "\n",
        "if not filtered:\n",
        "    print(\"No training samples found after filtering.\")\n",
        "else:\n",
        "    X_texts, y_labels = zip(*filtered)\n",
        "    X_texts, y_labels = list(X_texts), list(y_labels)\n",
        "\n",
        "    # Encode labels\n",
        "    label_enc = LabelEncoder()\n",
        "    y_enc = label_enc.fit_transform(y_labels)\n",
        "    num_classes = len(label_enc.classes_)\n",
        "\n",
        "    # Tokenize & pad sequences\n",
        "    max_words = 5000\n",
        "    max_len = 50\n",
        "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(X_texts)\n",
        "    X_seq = tokenizer.texts_to_sequences(X_texts)\n",
        "    X_pad = pad_sequences(X_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_pad, y_enc, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Convert to categorical\n",
        "    y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
        "    y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes=num_classes)\n",
        "\n",
        "    # --- Step 2: Define Tuned CNN ---\n",
        "    def build_cnn():\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Embedding(max_words, 128, input_length=max_len),\n",
        "            tf.keras.layers.Conv1D(256, 5, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.MaxPooling1D(2),\n",
        "            tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "            tf.keras.layers.GlobalMaxPooling1D(),\n",
        "            tf.keras.layers.Dropout(0.4),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "        model.compile(\n",
        "            loss='categorical_crossentropy',\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    # --- Step 3: Train CNN ---\n",
        "    cnn_model = build_cnn()\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "    history = cnn_model.fit(\n",
        "        X_train, y_train_cat,\n",
        "        validation_data=(X_val, y_val_cat),\n",
        "        epochs=8,\n",
        "        batch_size=64,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # --- Step 4: Evaluate ---\n",
        "    loss, acc = cnn_model.evaluate(X_val, y_val_cat, verbose=0)\n",
        "    print(f\"Tuned CNN Validation Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73o5z3jiRD2Q",
        "outputId": "a14459d9-f7a0-4928-bd9b-90de32c4367c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target property for prediction: 888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 - Deep Learning Model (Tuned CNN Only + Evaluation Metrics)\n",
        "# -------------------------------\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# --- Step 1: Prepare data ---\n",
        "target_prop = item_props[\"property\"].value_counts().index[0]\n",
        "print(\"Target property for prediction:\", target_prop)\n",
        "\n",
        "# Build item->latest value for target_prop\n",
        "prop_df = item_props[item_props[\"property\"] == target_prop].sort_values(\"timestamp\")\n",
        "item_latest_prop = prop_df.groupby(\"itemid\")[\"value\"].last().to_dict()\n",
        "\n",
        "X_texts, y_labels = [], []\n",
        "for _, row in events[events[\"event\"] == \"addtocart\"].iterrows():\n",
        "    user_hist = events[(events[\"visitorid\"] == row[\"visitorid\"]) &\n",
        "                       (events[\"timestamp\"] < row[\"timestamp\"]) &\n",
        "                       (events[\"event\"] == \"view\")]\n",
        "    tokens = [str(item_latest_prop.get(i, \"\")) for i in user_hist[\"itemid\"].values]\n",
        "    label = item_latest_prop.get(row[\"itemid\"])\n",
        "    if label and tokens:\n",
        "        X_texts.append(\" \".join(tokens))\n",
        "        y_labels.append(label)\n",
        "\n",
        "# Filter to frequent classes only\n",
        "counts = Counter(y_labels)\n",
        "min_freq = 10\n",
        "top_classes = {cls for cls, cnt in counts.items() if cnt >= min_freq}\n",
        "filtered = [(txt, lbl) for txt, lbl in zip(X_texts, y_labels) if lbl in top_classes]\n",
        "\n",
        "if not filtered:\n",
        "    print(\"No training samples found after filtering.\")\n",
        "else:\n",
        "    X_texts, y_labels = zip(*filtered)\n",
        "    X_texts, y_labels = list(X_texts), list(y_labels)\n",
        "\n",
        "    # Encode labels\n",
        "    label_enc = LabelEncoder()\n",
        "    y_enc = label_enc.fit_transform(y_labels)\n",
        "    num_classes = len(label_enc.classes_)\n",
        "\n",
        "    # Tokenize & pad sequences\n",
        "    max_words = 5000\n",
        "    max_len = 50\n",
        "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(X_texts)\n",
        "    X_seq = tokenizer.texts_to_sequences(X_texts)\n",
        "    X_pad = pad_sequences(X_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_pad, y_enc, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Convert to categorical\n",
        "    y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
        "    y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes=num_classes)\n",
        "\n",
        "    # --- Step 2: Define Tuned CNN ---\n",
        "    def build_cnn():\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Embedding(max_words, 128, input_length=max_len),\n",
        "            tf.keras.layers.Conv1D(256, 5, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.MaxPooling1D(2),\n",
        "            tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "            tf.keras.layers.GlobalMaxPooling1D(),\n",
        "            tf.keras.layers.Dropout(0.4),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "        model.compile(\n",
        "            loss='categorical_crossentropy',\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    # --- Step 3: Train CNN ---\n",
        "    cnn_model = build_cnn()\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "    history = cnn_model.fit(\n",
        "        X_train, y_train_cat,\n",
        "        validation_data=(X_val, y_val_cat),\n",
        "        epochs=8,\n",
        "        batch_size=64,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # --- Step 4: Evaluate ---\n",
        "    loss, acc = cnn_model.evaluate(X_val, y_val_cat, verbose=0)\n",
        "    print(f\"Tuned CNN Validation Accuracy: {acc:.4f}\")\n",
        "\n",
        "    # --- Step 5: Precision, Recall, F1 ---\n",
        "    y_pred_probs = cnn_model.predict(X_val)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    precision = precision_score(y_val, y_pred, average=\"weighted\")\n",
        "    recall = recall_score(y_val, y_pred, average=\"weighted\")\n",
        "    f1 = f1_score(y_val, y_pred, average=\"weighted\")\n",
        "\n",
        "    print(\"\\nClassification Metrics:\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    #print(\"\\nDetailed Report:\\n\", classification_report(y_val, y_pred, target_names=label_enc.classes_))\n"
      ],
      "metadata": {
        "id": "CbzBQae3UDQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Task 1: Save CNN Recommender Artifacts ---\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "# Save CNN model\n",
        "cnn_model.save(\"cnn_model.h5\")\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open(\"tokenizer.json\", \"w\") as f:\n",
        "    f.write(json.dumps(json.loads(tokenizer_json)))\n",
        "\n",
        "# Save label encoder\n",
        "with open(\"labelencoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(label_enc, f)\n",
        "\n",
        "print(\"✅ Task 1 artifacts saved: cnn_model.h5, tokenizer.json, labelencoder.pkl\")\n"
      ],
      "metadata": {
        "id": "RWthG7KASgm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK 2 - Abnormal User Detection"
      ],
      "metadata": {
        "id": "IqiX9N6tXIkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 - Abnormal User Detection (CNN Autoencoder + Visualization)\n",
        "# -------------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# --- Step 1: Build user-level features ---\n",
        "def build_user_features(events_df):\n",
        "    feats = []\n",
        "    for uid, g in events_df.groupby(\"visitorid\"):\n",
        "        total = len(g)\n",
        "        views = (g[\"event\"] == \"view\").sum()\n",
        "        adds = (g[\"event\"] == \"addtocart\").sum()\n",
        "        buys = (g[\"event\"] == \"transaction\").sum()\n",
        "        feats.append({\n",
        "            \"visitorid\": uid,\n",
        "            \"total_events\": total,\n",
        "            \"views\": views,\n",
        "            \"adds\": adds,\n",
        "            \"buys\": buys,\n",
        "            \"add_rate\": adds/total if total > 0 else 0,\n",
        "            \"conv_rate\": buys/total if total > 0 else 0\n",
        "        })\n",
        "    return pd.DataFrame(feats).set_index(\"visitorid\")\n",
        "\n",
        "user_feats = build_user_features(events)\n",
        "\n",
        "# --- Step 2: Scale features ---\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(user_feats)\n",
        "\n",
        "# Reshape for CNN (samples, timesteps, channels)\n",
        "X_cnn = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n",
        "\n",
        "# --- Step 3: Define CNN Autoencoder ---\n",
        "def build_cnn_autoencoder(seq_len):\n",
        "    input_layer = tf.keras.layers.Input(shape=(seq_len, 1))\n",
        "    x = tf.keras.layers.Conv1D(32, 2, activation='relu', padding='same')(input_layer)\n",
        "    x = tf.keras.layers.MaxPooling1D(2, padding='same')(x)\n",
        "    x = tf.keras.layers.Conv1D(16, 2, activation='relu', padding='same')(x)\n",
        "    x = tf.keras.layers.UpSampling1D(2)(x)\n",
        "    x = tf.keras.layers.Conv1D(32, 2, activation='relu', padding='same')(x)\n",
        "    decoded = tf.keras.layers.Conv1D(1, 2, activation='linear', padding='same')(x)\n",
        "    model = tf.keras.models.Model(inputs=input_layer, outputs=decoded)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
        "    return model\n",
        "\n",
        "cnn_autoencoder = build_cnn_autoencoder(X_cnn.shape[1])\n",
        "\n",
        "# --- Step 4: Train autoencoder ---\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "history = cnn_autoencoder.fit(\n",
        "    X_cnn, X_cnn,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- Step 5: Reconstruction error ---\n",
        "reconstructions = cnn_autoencoder.predict(X_cnn)\n",
        "mse = np.mean(np.power(X_cnn - reconstructions, 2), axis=(1, 2))\n",
        "threshold = np.percentile(mse, 98)  # top 2% anomaly threshold\n",
        "\n",
        "user_feats[\"reconstruction_error\"] = mse\n",
        "user_feats[\"outlier\"] = (mse > threshold).astype(int)\n",
        "print(f\"Detected outliers: {user_feats['outlier'].sum()} users\")"
      ],
      "metadata": {
        "id": "Vp8k-zUiVcbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Task 2: Save CNN Autoencoder Artifacts ---\n",
        "import pickle\n",
        "\n",
        "# Save CNN Autoencoder model\n",
        "cnn_autoencoder.save(\"cnn_ae.h5\")\n",
        "\n",
        "# Save scaler\n",
        "with open(\"scaler.pkl\", \"wb\") as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "print(\"✅ Task 2 artifacts saved: cnn_ae.h5, scaler.pkl\")"
      ],
      "metadata": {
        "id": "PyR5NWtEXGMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 6: Visualization (No Scientific Notation) ---\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(mse, bins=50, kde=True, color='blue')\n",
        "\n",
        "plt.axvline(threshold, color='red', linestyle='--', label=f'Threshold ({threshold:.4f})')\n",
        "\n",
        "plt.title(\"Reconstruction Error Distribution\")\n",
        "plt.xlabel(\"Reconstruction Error (MSE)\")\n",
        "plt.ylabel(\"Number of Users\")\n",
        "\n",
        "# Disable scientific notation on both axes\n",
        "plt.ticklabel_format(style='plain', axis='x')\n",
        "plt.ticklabel_format(style='plain', axis='y')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B8CMrvlRX8AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot for better view (No Scientific Notation)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.scatter(range(len(mse)), mse, c=(mse > threshold), cmap='coolwarm', s=10)\n",
        "\n",
        "plt.axhline(threshold, color='red', linestyle='--', label='Threshold')\n",
        "plt.title(\"User Reconstruction Error (Outlier Detection)\")\n",
        "plt.xlabel(\"User Index\")\n",
        "plt.ylabel(\"Reconstruction Error (MSE)\")\n",
        "\n",
        "# Disable scientific notation\n",
        "plt.ticklabel_format(style='plain', axis='x')\n",
        "plt.ticklabel_format(style='plain', axis='y')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "if9Ia0onYEy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merging Dataframes"
      ],
      "metadata": {
        "id": "o_ZVB9In2-VO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WoUt6sYe3GG5"
      }
    }
  ]
}